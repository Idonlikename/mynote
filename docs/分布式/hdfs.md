## `HDFS`-数据存储

`HDFS` 集群是建立在 `Hadoop` 集群之上的，由于 `HDFS` 是 `Hadoop` 最主要的**守护进程**基本可以认为是整个`Hadoop`的base。`HDFS` 和数据库相似。使用 `HDFS` 需要用 `HDFS` 客户端通过`Socket`连接到 `HDFS` 服务器实现文件系统的使用。

不支持随机写，但支持追加写。（因为是顺序写入）适合一次写入，多次读取。



### HDFS基本结构

其最基本的构成是一台主节点（`NameNode`）与多台数据节点（`DataNode`）

![img](https://pic3.zhimg.com/80/v2-a4355f6cb480af7c2f709e3c745f7dcc_720w.webp)

#### `Block`

物理磁盘中有块的概念，磁盘的物理Block是磁盘操作最小的单元，**读写操作均以Block为最小单元**，一般为512 Byte。文件系统在物理Block之上抽象了另一层概念，文件系统Block物理磁盘Block的整数倍。通常为几KB。`Hadoop`提供的`df`、`fsck`这类运维工具都是在文件系统的Block级别上进行操作。

`HDFS`的Block块比一般单机文件系统大得多，默认为`128M`。设置的如此之大是为了**最小化查找（seek）时间**，<u>控制定位文件与传输文件所用的时间比例</u>。比如定位到Block所需的时间为`10ms`，磁盘传输速度为`100M/s`。如果要将定位到Block所用时间占传输时间的比例控制1%，前面两个数据是一定的，我们需要 定位时间 = 0.01*block_size / 传输速度，则Block大小需要约`100M`。 但是如果Block设置过大，在`MapReduce`任务中，Map或者Reduce任务的个数就会变少。如果任务的个数小于集群机器数量，会使得作业运行效率很低。

`HDFS`的文件被拆分成block-sized的`chunk`，`chunk`作为**独立单元存储**。**比Block小的文件不会占用整个Block**，只会占据实际大小。

例如， 如果一个文件大小为`1M`，则在`HDFS`中只会占用`1M`的空间，而不是`128M`。



#### **`NameNode`（nn）** 

**主节点**或称**命名节点**，用于管理整个文件目录树（目录树在内存当中）。

`NameNode`如何进行文件目录管理呢？

可以理解为，`NameNode`存储着所有在`DataNode`所存储的文件的**元数据**，如其文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块所在的 `DataNode` 等。

与此同时，为了管理各个`DataNode`，协调各个`dn`，`nn`会监控每一个`dn`。为了让`nn`能够判断，`dn`有心跳机制每过一段时间发送心跳信号给`nn`，对于长时间无反应的`dn` `nn`便不再访问（说明网络问题或者`dn`宕机其他不可用问题）。这种机制很好地保证了，在使用时`dn`写入的**数据的完整可用**，对于数据节点的**故障检测**，**防止脑裂（脑裂也就是出现两个`NameNode`的情况，准确说是两个`NameNode`认为自己是Active）**。在`dn`写入上来说，`nn`会将写入的数据均匀负载，避免集中在某个节点导致过度访问。

命名节点 (`NameNode`) 是用于指挥其它节点存储的节点。任何一个"文件系统"(File System, FS) 都需要具备根据文件路径映射到文件的功能，命名节点就是用于**储存这些映射信息并提供映射服务的计算机**，在整个 `HDFS` 系统中扮演"管理员"的角色，因此一个 `HDFS` 集群中只有一个命名节点。client想要读写哪一个data都是由`nn`分配创建或查找`dn`实现。



#### **`DataNode`（dn）** 

**数据节点**，用于存储数据，以及对Client提供数据的读写服务。

数据节点 (`DataNode`) 使用来储存数据块的节点。当一个文件被命名节点承认并分块之后将会被储存到被分配的数据节点中去。数据节点具有储存数据、读写数据的功能，其中存储的数据块比较类似于硬盘中的"扇区"概念，是 `HDFS` **存储的基本单位**。

`dn`支持热扩展，即在整个`HDFS`使用过程中添加新的`dn` ，而无需重启。

为了保证我们的数据的安全，会进行数据Replication，也就是备份又称数据冗余来保护我们的数据。`FSDataOutputStream`(文件系统输出流)在写入数据在某一个数据节点时，这个节点会借助计算机网络实现在多个节点的备份。在完成备份之后`DataNode`才会告诉`nn`写入完成。

读写

![这里写图片描述](https://i-blog.csdnimg.cn/blog_migrate/086f55e2b446b9f68e99cd73ddaa24fd.png)

写数据

![这里写图片描述](https://i-blog.csdnimg.cn/blog_migrate/1d0d188130cd5ea0f233c2469ee6e909.png)

### 数据备份

因为Client访问文件必须经由`NN`，这使得`NN`在整个结构中都很重要。甚至`NN`是整个`HDFS`的主要性能瓶颈，提高`NN`的性能就能提高整个系统的响应速度。为了保证系统的响应速度，其实整个文件系统的**目录树都在内存中**。这反而导致了一个问题，一旦`NN`突然崩溃内存中的文件就会失去，那就会失去整个目录树，导致我们找不到`DataNode`中的文件了。

所以我们利用如下的两个结构帮助我们解决`NN`挂掉的问题。（在整个分布式系统中必须要考虑我们的任何节点会挂的可能）

#### edits

我们使用**日志**来帮助我们回复目录树，在任何引起`HDFS`改变的操作执行时，我们便会把操作写入**日志文件——edits**。当我们重启`NN`时，<u>edits会像一个脚本，读取那些操作执行</u>，将`NN`的目录树恢复到挂掉之前。

这引起了另一个问题，<u>如果我们的目录树更新的十分多，单单使用edits会导致恢复的时间异常漫长。</u>

为了解决这个问题我们引入`fsimage`——file system image **文件系统镜像**。也就是采用批处理的方法，当edits积累到一定程度我们则会把edits的操作并入`fsimage`中，保证我们的`fsimage`足够接近我们最新的目录树，当我们的`NN`挂掉之后，我们只需要直接将`fsimage`读入内存，再把`edits`执行，就可以恢复目录树。`fsimage`+`edits` = 内存元数据，保证了元数据持久化。



#### **`SecondaryNameNode`（2nn）** 

用来监控 `HDFS` 状态的辅助后台程序，每隔一段时间获取 `HDFS` 元数据的快照（`fsimage`+`edits`）。也就是备份节点。也就是负责更新`fsimage`的节点，启动时其会获取`NN`的`fsimage`再**定期（也就是每隔一段时间）**从`NN`获取`edits`来更新`2nn`当中的`fsimage`，`fsimage`与`edits`会合并出一个`fsimage.ckpt`返回给`NN`。

`2nn`仅保证了`nn`当中的镜像足够新，<u>具体用来重启的镜像实际仍存在在`NN`中</u>。

副命名节点 (`SecondaryNameNode`) 别名"次命名节点"，是命名节点的"秘书"。这个形容很贴切，因为它并**不能代替命名节点的工作**，无论命名节点是否有能力继续工作，因为在整个架构设计之初，`2nn`仅与`nn`通信，而不与`DN`由任何的关系。它主要负责分摊命名节点的压力、备份命名节点的状态并执行一些管理工作，如果命名节点要求它这样做的话。如果命名节点坏掉了，它也可以提供备份数据以恢复命名节点。**副命名节点可以有多个。**





### HA（High Available）架构

高度可用，消除单点故障问题，使得整个系统可用度变高。也就是使得系统变得强健。

#### `StandbyNameNode`

在`2nn`的基础上将`2nn`变为`Standby NameNode`，且我们**设置多个 `Standby NameNode`** （在`Hadoop3`当中）。或者说我们设置了多个`NameNode`而对于起作用的`NameNode`我们叫做 `Active NameNode` ，而用于备份的`NameNode` 叫做`Standby NameNode`。当`Active`挂了，`Standby`就会起作用变为`Active`，同时原先的`Active`变为`Standby`。在一般情况下，`Standby`做的都是`2nn`的作用。有所不同的是`Standby`**也有心跳机制**，也需要不断发送心跳信号给`Active`<u>以保证在启用备用节点能够快速代替。</u>

与`2nn`不同，`standby`**不再是定期地获取`edits`**而是<u>通过与`Active`共享的形式来获取`edits`</u>，一旦`edits`发生了更改，`Standby`都能立即获取，具体采用文件共享（采用局域网文件共享的协议）或日志共享。



#### `Zookeeper`

在有多个`stnn`的情况下，我们的系统会出现难以抉择出`New Active`的情形，此刻我们需要引入另外一个分布式软件——`zookeeper`，其提供**辅助协调工具**以帮助各个节点可以相互配合。甚至提供小数据存储。

在我们所提出的问题中，我们利用的特性进行解决：节点监视——会监视每一个节点，当节点被删除的时候会收到信息。以此帮助我们**判断`Active`挂了没**。不仅能够帮助我们判断`Active`挂了没，同时，它还可以**抉择出哪一个`Standby`作为下一个`Active`**，当获取`Active`挂了的信息之后，我们会让从`zookeeper`处**获得到一个同步锁的节点成为下一个“大哥”节点**。

`zookeeper`监控并选举`NN`

`zookeeper`本身也有`leader`和`follower` 由leader执行写操作，其他功能权限一致。当follower收到了写操作它会转发给leader，再允许follower执行写。

#### 脑裂

在之前我们已经提及了，脑裂就是两个`NN`认为自己都是AC的情况。客户端只会连接一个`ACNN`这个`ACNN`在`edits`所做的任何改变，另外一个`ACNN`都不会去读取进而导致了一个系统存在了多个目录树，导致了数据错乱等一系列问题。

还可以通过编写fencing来避免脑裂。

`NFS`与`QJM`两种不同的共享edits方式，使得后者不需要担心脑裂。前者依靠网络，后者是共享文件。

### 数据副本机制

我们提及过，对于一个分布式系统而言，我们是极不放心硬件的稳定性的。而`Hadoop`在设计时考虑到数据的安全与高效，数据文件默认在`HDFS`上存放三份，存储策略为**本地一份**，**同机架内其它某一节点**上一份，**不同机架的某一节点**上一份。这样如果本地数据损坏，节点可以从同一机架内的相邻节点拿到数据，速度肯定比从跨机架节点上拿数据要快；同时，如果整个机架的网络出现异常，也能保证在其它机架的节点上找到数据。确保了数据获取的高效。

而对于一个服务器集群而言我们有同台服务器的概念（在同一台主机的硬件上），同一个机架（rack，共用很多资源，包括但不限于网络、电源）。

对于同一个机架和不同机架的两个服务器，同一个机架的服务器可能因为使用同样的子网网络连接更直接，或使用同一根光缆，或者有更近的物理距离。因而交互的速度更快。

#### 副本距离

**同一台服务器**的距离为0
$$
Distance(Rack1/D1, Rack1/D1)=0
$$
**同一机架不同的服务器**距离为2
$$
Distance(Rack1/D1, Rack1/D3)=2
$$
**不同机架的服务器**距离为4
$$
Distance(Rack1/D1, Rack2/D1)=4
$$


#### 副本放置策略

第一个副本（也就是原件）在本地机器

第二个副本在远端机架的节点（也不一定是不同机架）

第三个副本看之前的两个副本是否在同一机架，如果是则选择其他机架，否则选择和第一个副本相同机架的不同节点（确保前二、三副本在不同的机架上，使得在获取副本的时候有取舍）

第四个及以上的副本，随机选择副本存放位置。